---
title: "Preliminary Report"
author: "Derwin McGeary"
date: "20 December 2015"
output: html_document
---
## Introduction
This is a preliminary report on the datasets for the Capstone Project. I will give a quick summary of the contents and nature of the data.

## The basics

```{r, echo=FALSE}
suppressWarnings({
  library("tm")
  library("ggplot2")
  library("knitr")
})
```

We have three large files of English text, from three sources: News articles, blogs, and tweets. Each line contains one article, blog or tweet. We have a fairly large amount of data, 2360148 tweets, 899288 blogs, and 1010242 news articles.

### The Tweets
```{r, echo=FALSE}
suppressWarnings(tweets <- readLines("final/en_US/en_US.twitter.txt"))
tlengths <- nchar(tweets)
tsummary <- summary(tlengths)
ggplot(as.data.frame(tlengths), aes(x=tlengths)) + 
  ggtitle("Distribution of Tweet Lengths") + xlab("Character Count") +
    geom_histogram(binwidth=1, colour="white", fill="#55acee") + theme_bw()
print(tsummary)
rm(tweets)
```

This plot shows that the longest tweet is `r max(tlengths)` characters, and it we can see that there is a hard limit as that is the most popular length, although there is a wide distribution of lengths.


### The News
```{r, echo=FALSE}
news <- readLines("final/en_US/en_US.news.txt")
nwords <- sapply(gregexpr("[[:alpha:]]+", news), function(x) sum(x > 0))
keep <- nwords<1000
news <- news[keep]
nwords <- nwords[keep]
keep <- nwords>4
news <- news[keep]
nwords <- nwords[keep]
nlengths <- nchar(news)
nsummary <- summary(nlengths)
nwsummary <- summary(nwords)
ggplot(as.data.frame(nwords), aes(x=nwords)) + 
  ggtitle("Distribution of News Article Lengths") + xlab("Word Count")+ 
    geom_histogram(binwidth=1, colour="#bb1919", fill="#bb1919") + theme_bw()
print(nsummary)
rm(news)
```

We have filtered some very long lists of names and items with four words or less (which upon inspection turned out to be meaningless text fragments). The longest article is `r max(nwords)` words, and the average is `r mean(nwords)`

### The Blogs

```{r, echo=FALSE}
blogs <- readLines("final/en_US/en_US.blogs.txt")
blengths <- nchar(blogs)
bwords <- sapply(gregexpr("[[:alpha:]]+", blogs), function(x) sum(x > 0))
bsummary <- summary(blengths)
ggplot(as.data.frame(bwords), aes(x=bwords)) + 
  ggtitle("Distribution of Blog Post Lengths") + xlab("Word Count") +
    geom_histogram(binwidth=1, colour="black", fill="white") + theme_bw()
print(bsummary)
bwsummary <- summary(bwords)
rm(blogs)
```

This dataset is highly skewed, as only `r length(bwords[bwords>500])` articles (`r round(length(bwords[bwords>500])/length(bwords) * 100,3)`%) are longer than 500 words. The average is `r round(mean(bwords))` words.